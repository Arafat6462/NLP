{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5a0f33f",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Tokenization is the process of breaking down the given text in natural language processing into the smallest unit in a sentence called a token. Punctuation marks, words, and numbers can be considered tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fff9bbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Hi Everyone! This is Arafat Hossain. We are learning Natural Language Processing. We reached 1000 views.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd8abccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'Everyone!',\n",
       " 'This',\n",
       " 'is',\n",
       " 'Arafat',\n",
       " 'Hossain.',\n",
       " 'We',\n",
       " 'are',\n",
       " 'learning',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing.',\n",
       " 'We',\n",
       " 'reached',\n",
       " '1000',\n",
       " 'views.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba3cf3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1e03d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi Everyone!',\n",
       " 'This is Arafat Hossain.',\n",
       " 'We are learning Natural Language Processing.',\n",
       " 'We reached 1000 views.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the text into sentences\n",
    "sent_tokens = sent_tokenize(text)\n",
    "sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8f7fc56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'Everyone',\n",
       " '!',\n",
       " 'This',\n",
       " 'is',\n",
       " 'Arafat',\n",
       " 'Hossain',\n",
       " '.',\n",
       " 'We',\n",
       " 'are',\n",
       " 'learning',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " '.',\n",
       " 'We',\n",
       " 'reached',\n",
       " '1000',\n",
       " 'views',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the text into word\n",
    "word_tokens = word_tokenize(text)\n",
    "word_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e00c1f",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "Stemming is the process of finding the root of words. A word stem need not be the same root as a dictionary-based morphological root, it just is an equal to or smaller form of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "096540e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4596926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = ('eats')\n",
    "ps.stem(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53ebede4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = ('eating')\n",
    "ps.stem(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2265c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eaten'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = ('eaten')\n",
    "ps.stem(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56b5bbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Hi Everyone! This is Arafat Hossain. We are learning Natural Language Processing. We reached 1000 views.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "298f38bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'Everyone',\n",
       " '!',\n",
       " 'This',\n",
       " 'is',\n",
       " 'Arafat',\n",
       " 'Hossain',\n",
       " '.',\n",
       " 'We',\n",
       " 'are',\n",
       " 'learning',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " '.',\n",
       " 'We',\n",
       " 'reached',\n",
       " '1000',\n",
       " 'views',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens = word_tokenize(text)\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e27eb9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi everyon ! thi is arafat hossain . we are learn natur languag process . we reach 1000 view .'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_sentence = \" \".join(ps.stem(word) for word in word_tokens)\n",
    "stemmed_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c13812",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "Lemmatization is the process of finding the form of the related word in the dictionary. It is different from Stemming. It involves longer processes to calculate than Stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed82cb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f53039a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'worker'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('workers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd3b3126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3763901e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('this')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46208ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stripe'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('stripes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8398543f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'strip'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('stripes', 'v') # v for verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ca74aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stripe'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('stripes', 'n') # n for noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96c086e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Hi Everyone! This is Arafats Hossain. We are learning Natural Language Processing. We reached 1000 views.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eca5d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9acb94b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Everyone ! This is Arafats Hossain . We are learning Natural Language Processing . We reached 1000 view .'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_sentence = \" \".join(lemmatizer.lemmatize(word) for word in word_tokens)\n",
    "lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "198dd597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi everyone ! this is arafat hossain . we are learning natural language processing . we reached 1000 view .'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_sentence = \" \".join(lemmatizer.lemmatize(word.lower()) for word in word_tokens)\n",
    "lemmatized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7109cba0",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging (POS)\n",
    "Part of Speech Tagging is a process of converting a sentence to forms — list of words, list of tuples (where each tuple is having a form (word, tag)). The tag in case of is a part-of-speech tag, and signifies whether the word is a noun, adjective, verb, and so on.\n",
    "\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0391ca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e3c3af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/arafat/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23f7349b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fighting', 'VBG')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(['fighting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cce1664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Hi Everyone! This is Arafat Hossain. We are learning Natural Language Processing. We reached 1000 views.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5a7b6a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-19T07:38:34.417952Z",
     "start_time": "2022-09-19T07:38:34.017338Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m word_tokensr \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m(text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "word_tokensr = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8db5ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hi', 'NNP'),\n",
       " ('Everyone', 'NN'),\n",
       " ('!', '.'),\n",
       " ('This', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('Arafats', 'NNP'),\n",
       " ('Hossain', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('We', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('learning', 'VBG'),\n",
       " ('Natural', 'NNP'),\n",
       " ('Language', 'NNP'),\n",
       " ('Processing', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('We', 'PRP'),\n",
       " ('reached', 'VBD'),\n",
       " ('1000', 'CD'),\n",
       " ('views', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfcbad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb1b44ea",
   "metadata": {},
   "source": [
    "# Text Preprocessing (Clean Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf3fe169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                        tweet\n",
       "0                       @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run\n",
       "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked\n",
       "2                                                                                                         bihday your majesty\n",
       "3                                      #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦  \n",
       "4                                                                                      factsguide: society now    #motivation"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "df = pd.read_csv('Twitter Sentiments.csv')\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "# drop the columns\n",
    "df = df.drop(columns=['id', 'label'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f0d4d0",
   "metadata": {},
   "source": [
    "# Converting to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef2121f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                        tweet  \\\n",
       "0                       @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run   \n",
       "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked   \n",
       "2                                                                                                         bihday your majesty   \n",
       "3                                      #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦     \n",
       "4                                                                                      factsguide: society now    #motivation   \n",
       "\n",
       "                                                                                                                   clean_text  \n",
       "0                       @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run  \n",
       "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked  \n",
       "2                                                                                                         bihday your majesty  \n",
       "3                                      #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦    \n",
       "4                                                                                      factsguide: society now    #motivation  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['tweet'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76586bc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15238</th>\n",
       "      <td>#kuturns15   bihdayð #loveyouð</td>\n",
       "      <td>#kuturns15   bihdayð #loveyouð</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721</th>\n",
       "      <td>@user pre-ordered @user #trulyhappybaby book today ð¶ð¼   #newmummy</td>\n",
       "      <td>@user pre-ordered @user #trulyhappybaby book today ð¶ð¼   #newmummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23283</th>\n",
       "      <td>another #macro of the #sagopalm #tree it's fascinating to me! #green #healthy   #plantsâ¦</td>\n",
       "      <td>another #macro of the #sagopalm #tree it's fascinating to me! #green #healthy   #plantsâ¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8360</th>\n",
       "      <td>choose to be happy, no matter what your circumstances are! it will free you! #smile</td>\n",
       "      <td>choose to be happy, no matter what your circumstances are! it will free you! #smile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2667</th>\n",
       "      <td>i'm maia! #fun #daughter #princess   #toddler #maryland #silverspring #model @user</td>\n",
       "      <td>i'm maia! #fun #daughter #princess   #toddler #maryland #silverspring #model @user</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                             tweet  \\\n",
       "15238                                                        #kuturns15   bihdayð #loveyouð    \n",
       "1721                       @user pre-ordered @user #trulyhappybaby book today ð¶ð¼   #newmummy   \n",
       "23283  another #macro of the #sagopalm #tree it's fascinating to me! #green #healthy   #plantsâ¦    \n",
       "8360        choose to be happy, no matter what your circumstances are! it will free you! #smile      \n",
       "2667           i'm maia! #fun #daughter #princess   #toddler #maryland #silverspring #model @user    \n",
       "\n",
       "                                                                                        clean_text  \n",
       "15238                                                        #kuturns15   bihdayð #loveyouð   \n",
       "1721                       @user pre-ordered @user #trulyhappybaby book today ð¶ð¼   #newmummy  \n",
       "23283  another #macro of the #sagopalm #tree it's fascinating to me! #green #healthy   #plantsâ¦   \n",
       "8360        choose to be happy, no matter what your circumstances are! it will free you! #smile     \n",
       "2667           i'm maia! #fun #daughter #princess   #toddler #maryland #silverspring #model @user   "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(frac=1).head() # shaffaling the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0a5e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2c6630b",
   "metadata": {},
   "source": [
    "# Removal of Punctuations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d072f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "031598dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    punctuations = string.punctuation\n",
    "    return text.translate(str.maketrans('', '', punctuations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d690f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "      <td>user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction   run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "      <td>user user thanks for lyft credit i cant use cause they dont offer wheelchair vans in pdx    disapointed getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "      <td>model   i love u take with u all the time in urð± ðððð",
       "ð¦ð¦ð¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society now    motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                        tweet  \\\n",
       "0                       @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run   \n",
       "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked   \n",
       "2                                                                                                         bihday your majesty   \n",
       "3                                      #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦     \n",
       "4                                                                                      factsguide: society now    #motivation   \n",
       "\n",
       "                                                                                                           clean_text  \n",
       "0                  user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction   run  \n",
       "1  user user thanks for lyft credit i cant use cause they dont offer wheelchair vans in pdx    disapointed getthanked  \n",
       "2                                                                                                 bihday your majesty  \n",
       "3                                  model   i love u take with u all the time in urð± ðððð\n",
       "ð¦ð¦ð¦    \n",
       "4                                                                                factsguide society now    motivation  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['clean_text'].apply(lambda x: remove_punctuation(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451f19cf",
   "metadata": {},
   "source": [
    "# Removal of Stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "799ee315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\", \".join(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91cb442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in text.split() if word not in STOPWORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2bc6d1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "      <td>user father dysfunctional selfish drags kids dysfunction run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "      <td>user user thanks lyft credit cant use cause dont offer wheelchair vans pdx disapointed getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "      <td>model love u take u time urð± ðððð ð¦ð¦ð¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                        tweet  \\\n",
       "0                       @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run   \n",
       "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked   \n",
       "2                                                                                                         bihday your majesty   \n",
       "3                                      #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦     \n",
       "4                                                                                      factsguide: society now    #motivation   \n",
       "\n",
       "                                                                                          clean_text  \n",
       "0                                       user father dysfunctional selfish drags kids dysfunction run  \n",
       "1  user user thanks lyft credit cant use cause dont offer wheelchair vans pdx disapointed getthanked  \n",
       "2                                                                                     bihday majesty  \n",
       "3                                       model love u take u time urð± ðððð ð¦ð¦ð¦  \n",
       "4                                                                      factsguide society motivation  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['clean_text'].apply(lambda x: remove_stopwords(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6204e8",
   "metadata": {},
   "source": [
    "# Removal of Frequent Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3150238a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('user', 17473),\n",
       " ('love', 2647),\n",
       " ('day', 2198),\n",
       " ('happy', 1663),\n",
       " ('amp', 1582),\n",
       " ('im', 1139),\n",
       " ('u', 1136),\n",
       " ('time', 1110),\n",
       " ('life', 1086),\n",
       " ('like', 1042)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_count = Counter()\n",
    "\n",
    "for text in df['clean_text']:\n",
    "    for word in text.split():\n",
    "        word_count[word] +=1\n",
    "\n",
    "word_count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9902d266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'day', 'love', 'user'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FREQUENT_WORDS = set(word for (word, wc) in word_count.most_common(3))\n",
    "FREQUENT_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "60041d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FREQUENT_WORDS = set(word for (word, wc) in word_count.most_common(1))\n",
    "FREQUENT_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "856c6132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_freq_word(text):\n",
    "    return \" \".join([word for word in text.split() if word not in FREQUENT_WORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "74c065c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "      <td>father dysfunctional selfish drags kids dysfunction run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "      <td>thanks lyft credit cant use cause dont offer wheelchair vans pdx disapointed getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "      <td>model love u take u time urð± ðððð ð¦ð¦ð¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                        tweet  \\\n",
       "0                       @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run   \n",
       "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked   \n",
       "2                                                                                                         bihday your majesty   \n",
       "3                                      #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦     \n",
       "4                                                                                      factsguide: society now    #motivation   \n",
       "\n",
       "                                                                                clean_text  \n",
       "0                                  father dysfunctional selfish drags kids dysfunction run  \n",
       "1  thanks lyft credit cant use cause dont offer wheelchair vans pdx disapointed getthanked  \n",
       "2                                                                           bihday majesty  \n",
       "3                             model love u take u time urð± ðððð ð¦ð¦ð¦  \n",
       "4                                                            factsguide society motivation  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['clean_text'].apply(lambda x: remove_freq_word(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79216b23",
   "metadata": {},
   "source": [
    "## Removal of Rare Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b73939c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'airwaves',\n",
       " 'carnt',\n",
       " 'chisolm',\n",
       " 'ibizabringitonmallorcaholidayssummer',\n",
       " 'isz',\n",
       " 'mantle',\n",
       " 'shirley',\n",
       " 'youuuð\\x9f\\x98\\x8dð\\x9f\\x98\\x8dð\\x9f\\x98\\x8dð\\x9f\\x98\\x8dð\\x9f\\x98\\x8dð\\x9f\\x98\\x8dð\\x9f\\x98\\x8dð\\x9f\\x98\\x8dð\\x9f\\x98\\x8dâ\\x9d¤ï¸\\x8f',\n",
       " 'ð\\x9f\\x99\\x8fð\\x9f\\x8f¼ð\\x9f\\x8d¹ð\\x9f\\x98\\x8eð\\x9f\\x8eµ'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RARE_WORDS = set(word for (word, wc) in word_count.most_common()[:-10:-1])\n",
    "RARE_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7cdc8f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('airwaves', 1),\n",
       " ('carnt', 1),\n",
       " ('chisolm', 1),\n",
       " ('ibizabringitonmallorcaholidayssummer', 1),\n",
       " ('isz', 1),\n",
       " ('mantle', 1),\n",
       " ('shirley', 1),\n",
       " ('youuuð\\x9f\\x98\\x8dð\\x9f\\x98\\x8dð\\x9f\\x98\\x8dð\\x9f\\x98\\x8dð\\x9f\\x98\\x8dð\\x9f\\x98\\x8dð\\x9f\\x98\\x8dð\\x9f\\x98\\x8dð\\x9f\\x98\\x8dâ\\x9d¤ï¸\\x8f',\n",
       "  1),\n",
       " ('ð\\x9f\\x99\\x8fð\\x9f\\x8f¼ð\\x9f\\x8d¹ð\\x9f\\x98\\x8eð\\x9f\\x8eµ', 1)}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RARE_WORDS = set((word, wc) for (word, wc) in word_count.most_common()[:-10:-1])\n",
    "RARE_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "92e000c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rare_words(text):\n",
    "    return \" \".join([word for word in text.split() if word not in RARE_WORDS])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a7f1cfa4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "      <td>father dysfunctional selfish drags kids dysfunction run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "      <td>thanks lyft credit cant use cause dont offer wheelchair vans pdx disapointed getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "      <td>model love u take u time urð± ðððð ð¦ð¦ð¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                        tweet  \\\n",
       "0                       @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run   \n",
       "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked   \n",
       "2                                                                                                         bihday your majesty   \n",
       "3                                      #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦     \n",
       "4                                                                                      factsguide: society now    #motivation   \n",
       "\n",
       "                                                                                clean_text  \n",
       "0                                  father dysfunctional selfish drags kids dysfunction run  \n",
       "1  thanks lyft credit cant use cause dont offer wheelchair vans pdx disapointed getthanked  \n",
       "2                                                                           bihday majesty  \n",
       "3                             model love u take u time urð± ðððð ð¦ð¦ð¦  \n",
       "4                                                            factsguide society motivation  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['clean_text'].apply(lambda x: remove_rare_words(x))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218e802a",
   "metadata": {},
   "source": [
    "# Removal of Special characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b278b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_spl_chars(text):\n",
    "    text = re.sub('[^a-zA-Z0-9]', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "671642f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "      <td>father dysfunctional selfish drags kids dysfunction run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "      <td>thanks lyft credit cant use cause dont offer wheelchair vans pdx disapointed getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "      <td>model love u take u time ur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                        tweet  \\\n",
       "0                       @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run   \n",
       "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked   \n",
       "2                                                                                                         bihday your majesty   \n",
       "3                                      #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦     \n",
       "4                                                                                      factsguide: society now    #motivation   \n",
       "\n",
       "                                                                                clean_text  \n",
       "0                                  father dysfunctional selfish drags kids dysfunction run  \n",
       "1  thanks lyft credit cant use cause dont offer wheelchair vans pdx disapointed getthanked  \n",
       "2                                                                           bihday majesty  \n",
       "3                                                             model love u take u time ur   \n",
       "4                                                            factsguide society motivation  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['clean_text'].apply(lambda x: remove_spl_chars(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5a6dde",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "09edf508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a272d1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>stemmed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "      <td>father dysfunctional selfish drags kids dysfunction run</td>\n",
       "      <td>father dysfunct selfish drag kid dysfunct run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "      <td>thanks lyft credit cant use cause dont offer wheelchair vans pdx disapointed getthanked</td>\n",
       "      <td>thank lyft credit cant use caus dont offer wheelchair van pdx disapoint getthank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday majesty</td>\n",
       "      <td>bihday majesti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "      <td>model love u take u time ur</td>\n",
       "      <td>model love u take u time ur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society motivation</td>\n",
       "      <td>factsguid societi motiv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                        tweet  \\\n",
       "0                       @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run   \n",
       "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked   \n",
       "2                                                                                                         bihday your majesty   \n",
       "3                                      #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦     \n",
       "4                                                                                      factsguide: society now    #motivation   \n",
       "\n",
       "                                                                                clean_text  \\\n",
       "0                                  father dysfunctional selfish drags kids dysfunction run   \n",
       "1  thanks lyft credit cant use cause dont offer wheelchair vans pdx disapointed getthanked   \n",
       "2                                                                           bihday majesty   \n",
       "3                                                             model love u take u time ur    \n",
       "4                                                            factsguide society motivation   \n",
       "\n",
       "                                                                       stemmed_text  \n",
       "0                                     father dysfunct selfish drag kid dysfunct run  \n",
       "1  thank lyft credit cant use caus dont offer wheelchair van pdx disapoint getthank  \n",
       "2                                                                    bihday majesti  \n",
       "3                                                       model love u take u time ur  \n",
       "4                                                           factsguid societi motiv  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['stemmed_text'] = df['clean_text'].apply(lambda x: stem_words(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6461909",
   "metadata": {},
   "source": [
    "# Lemmatization & POS Tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e682d22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lammatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    #find pos tags\n",
    "    \n",
    "    pos_text = pos_tag(text.split())\n",
    "    return \" \".join(lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e35fb61d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "852ccc2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "      <td>father dysfunctional selfish drags kids dysfunction run</td>\n",
       "      <td>father dysfunct selfish drag kid dysfunct run</td>\n",
       "      <td>father dysfunctional selfish drag kid dysfunction run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "      <td>thanks lyft credit cant use cause dont offer wheelchair vans pdx disapointed getthanked</td>\n",
       "      <td>thank lyft credit cant use caus dont offer wheelchair van pdx disapoint getthank</td>\n",
       "      <td>thanks lyft credit cant use cause dont offer wheelchair van pdx disapointed getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday majesty</td>\n",
       "      <td>bihday majesti</td>\n",
       "      <td>bihday majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "      <td>model love u take u time ur</td>\n",
       "      <td>model love u take u time ur</td>\n",
       "      <td>model love u take u time ur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society motivation</td>\n",
       "      <td>factsguid societi motiv</td>\n",
       "      <td>factsguide society motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                        tweet  \\\n",
       "0                       @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run   \n",
       "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked   \n",
       "2                                                                                                         bihday your majesty   \n",
       "3                                      #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦     \n",
       "4                                                                                      factsguide: society now    #motivation   \n",
       "\n",
       "                                                                                clean_text  \\\n",
       "0                                  father dysfunctional selfish drags kids dysfunction run   \n",
       "1  thanks lyft credit cant use cause dont offer wheelchair vans pdx disapointed getthanked   \n",
       "2                                                                           bihday majesty   \n",
       "3                                                             model love u take u time ur    \n",
       "4                                                            factsguide society motivation   \n",
       "\n",
       "                                                                       stemmed_text  \\\n",
       "0                                     father dysfunct selfish drag kid dysfunct run   \n",
       "1  thank lyft credit cant use caus dont offer wheelchair van pdx disapoint getthank   \n",
       "2                                                                    bihday majesti   \n",
       "3                                                       model love u take u time ur   \n",
       "4                                                           factsguid societi motiv   \n",
       "\n",
       "                                                                          lemmatized_text  \n",
       "0                                   father dysfunctional selfish drag kid dysfunction run  \n",
       "1  thanks lyft credit cant use cause dont offer wheelchair van pdx disapointed getthanked  \n",
       "2                                                                          bihday majesty  \n",
       "3                                                             model love u take u time ur  \n",
       "4                                                           factsguide society motivation  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lemmatized_text'] = df['clean_text'].apply(lambda x: lemmatize_words(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1ed08686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23919</th>\n",
       "      <td>@user   fathers day. have a memorable day. from all 1950's waspi women who bore you, raised you, fed you, cared for you,â¦</td>\n",
       "      <td>fathers day memorable day 1950s waspi women bore raised fed cared you</td>\n",
       "      <td>father day memor day 1950 waspi women bore rais fed care you</td>\n",
       "      <td>father day memorable day 1950s waspi woman bore raised fed care you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6948</th>\n",
       "      <td>@user @user #ineedtoconsult  bro t, i have lot of un-answered questions about my life but the only person to be consultâ¦</td>\n",
       "      <td>ineedtoconsult bro lot unanswered questions life person consult</td>\n",
       "      <td>ineedtoconsult bro lot unansw question life person consult</td>\n",
       "      <td>ineedtoconsult bro lot unanswered question life person consult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td>@user don't miss the chance of your life to be  ! *happy divorce*  e-book &amp;amp; paperback</td>\n",
       "      <td>dont miss chance life happy divorce ebook amp paperback</td>\n",
       "      <td>dont miss chanc life happi divorc ebook amp paperback</td>\n",
       "      <td>dont miss chance life happy divorce ebook amp paperback</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21867</th>\n",
       "      <td>i'm afraid imma need longer sessions than my normal sessions. loool   #enjoyment #gymtime</td>\n",
       "      <td>im afraid imma need longer sessions normal sessions loool enjoyment gymtime</td>\n",
       "      <td>im afraid imma need longer session normal session loool enjoy gymtim</td>\n",
       "      <td>im afraid imma need long session normal session loool enjoyment gymtime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4685</th>\n",
       "      <td>we have to make a decision to be   on the inside now, to magnetize a life of happiness on the outside.</td>\n",
       "      <td>make decision inside magnetize life happiness outside</td>\n",
       "      <td>make decis insid magnet life happi outsid</td>\n",
       "      <td>make decision inside magnetize life happiness outside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29239</th>\n",
       "      <td>well,these sewer rats don't take a cent from me,....i don't use to watch \"football\",......but now,i hate it!</td>\n",
       "      <td>wellthese sewer rats dont take cent mei dont use watch footballbut nowi hate</td>\n",
       "      <td>wellthes sewer rat dont take cent mei dont use watch footballbut nowi hate</td>\n",
       "      <td>wellthese sewer rat dont take cent mei dont use watch footballbut nowi hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30269</th>\n",
       "      <td>yeah!!!!!!!!!!!   #allblacks #allblackeverything #nzlvwal</td>\n",
       "      <td>yeah allblacks allblackeverything nzlvwal</td>\n",
       "      <td>yeah allblack allblackeveryth nzlvwal</td>\n",
       "      <td>yeah allblacks allblackeverything nzlvwal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11164</th>\n",
       "      <td>nonegativeselftalk: #money &amp;amp; status can't cure low self-esteem or make you  . learn why in this post. â¦</td>\n",
       "      <td>nonegativeselftalk money amp status cant cure low selfesteem make learn post</td>\n",
       "      <td>nonegativeselftalk money amp statu cant cure low selfesteem make learn post</td>\n",
       "      <td>nonegativeselftalk money amp status cant cure low selfesteem make learn post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9416</th>\n",
       "      <td>â¡ï¸ âcharles paladino's racist comments spark calls for resignationâ</td>\n",
       "      <td>charles paladinos racist comments spark calls resignation</td>\n",
       "      <td>charl paladino racist comment spark call resign</td>\n",
       "      <td>charles paladinos racist comment spark call resignation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5784</th>\n",
       "      <td>are you #black &amp;amp; feel like the  are stomping â¦ you? listen  #retweet #tampa #miamiâ¦</td>\n",
       "      <td>black amp feel like stomping listen retweet tampa miami</td>\n",
       "      <td>black amp feel like stomp listen retweet tampa miami</td>\n",
       "      <td>black amp feel like stomp listen retweet tampa miami</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                              tweet  \\\n",
       "23919   @user   fathers day. have a memorable day. from all 1950's waspi women who bore you, raised you, fed you, cared for you,â¦   \n",
       "6948     @user @user #ineedtoconsult  bro t, i have lot of un-answered questions about my life but the only person to be consultâ¦   \n",
       "2075                                      @user don't miss the chance of your life to be  ! *happy divorce*  e-book &amp; paperback   \n",
       "21867                                     i'm afraid imma need longer sessions than my normal sessions. loool   #enjoyment #gymtime   \n",
       "4685                        we have to make a decision to be   on the inside now, to magnetize a life of happiness on the outside.    \n",
       "29239                 well,these sewer rats don't take a cent from me,....i don't use to watch \"football\",......but now,i hate it!    \n",
       "30269                                                                   yeah!!!!!!!!!!!   #allblacks #allblackeverything #nzlvwal     \n",
       "11164                nonegativeselftalk: #money &amp; status can't cure low self-esteem or make you  . learn why in this post. â¦    \n",
       "9416                                                  â¡ï¸ âcharles paladino's racist comments spark calls for resignationâ     \n",
       "5784                                   are you #black &amp; feel like the  are stomping â¦ you? listen  #retweet #tampa #miamiâ¦    \n",
       "\n",
       "                                                                          clean_text  \\\n",
       "23919         fathers day memorable day 1950s waspi women bore raised fed cared you    \n",
       "6948                ineedtoconsult bro lot unanswered questions life person consult    \n",
       "2075                         dont miss chance life happy divorce ebook amp paperback   \n",
       "21867    im afraid imma need longer sessions normal sessions loool enjoyment gymtime   \n",
       "4685                           make decision inside magnetize life happiness outside   \n",
       "29239   wellthese sewer rats dont take cent mei dont use watch footballbut nowi hate   \n",
       "30269                                      yeah allblacks allblackeverything nzlvwal   \n",
       "11164  nonegativeselftalk money amp status cant cure low selfesteem make learn post    \n",
       "9416                      charles paladinos racist comments spark calls resignation    \n",
       "5784                        black amp feel like stomping listen retweet tampa miami    \n",
       "\n",
       "                                                                      stemmed_text  \\\n",
       "23919                 father day memor day 1950 waspi women bore rais fed care you   \n",
       "6948                    ineedtoconsult bro lot unansw question life person consult   \n",
       "2075                         dont miss chanc life happi divorc ebook amp paperback   \n",
       "21867         im afraid imma need longer session normal session loool enjoy gymtim   \n",
       "4685                                     make decis insid magnet life happi outsid   \n",
       "29239   wellthes sewer rat dont take cent mei dont use watch footballbut nowi hate   \n",
       "30269                                        yeah allblack allblackeveryth nzlvwal   \n",
       "11164  nonegativeselftalk money amp statu cant cure low selfesteem make learn post   \n",
       "9416                               charl paladino racist comment spark call resign   \n",
       "5784                          black amp feel like stomp listen retweet tampa miami   \n",
       "\n",
       "                                                                    lemmatized_text  \n",
       "23919           father day memorable day 1950s waspi woman bore raised fed care you  \n",
       "6948                 ineedtoconsult bro lot unanswered question life person consult  \n",
       "2075                        dont miss chance life happy divorce ebook amp paperback  \n",
       "21867       im afraid imma need long session normal session loool enjoyment gymtime  \n",
       "4685                          make decision inside magnetize life happiness outside  \n",
       "29239   wellthese sewer rat dont take cent mei dont use watch footballbut nowi hate  \n",
       "30269                                     yeah allblacks allblackeverything nzlvwal  \n",
       "11164  nonegativeselftalk money amp status cant cure low selfesteem make learn post  \n",
       "9416                        charles paladinos racist comment spark call resignation  \n",
       "5784                           black amp feel like stomp listen retweet tampa miami  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(frac=1).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5eec044a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420579"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "word_count =0\n",
    "\n",
    "for text in df['tweet']:\n",
    "    for word in text.split():\n",
    "        word_count +=1\n",
    "\n",
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "303383e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254461"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count1 =0 \n",
    "\n",
    "for text in df['clean_text']:\n",
    "    for word in text.split():\n",
    "        word_count1 +=1\n",
    "word_count1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0c8f14eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254461"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "word_count2 = 0\n",
    "\n",
    "for text in df['stemmed_text']:\n",
    "    for word in text.split():\n",
    "        word_count2 +=1\n",
    "\n",
    "word_count2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78082596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473d9759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114603df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a188b695",
   "metadata": {},
   "source": [
    "# Removal of URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b20151fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"https://www.abcd.net is the URL of abcd\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5a701f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+','',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a84a4da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' is the URL of abcd'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_url(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad36911",
   "metadata": {},
   "source": [
    "# Removal of HTML Tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1ee5e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"<html><body> <h1>Arafat Hossain</h1> <p>This is a sample text</p> </body></html>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4542a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_html_tags(text):\n",
    "    return re.sub(r'<.*?>', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8b286447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Arafat Hossain This is a sample text '"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_html_tags(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13288044",
   "metadata": {},
   "source": [
    "# Spelling Correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1aaf9f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspellchecker in /home/arafat/anaconda3/lib/python3.9/site-packages (0.7.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b23e5f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'natur is a beuty'\n",
    "text = 'helllo, m am Arafat. i\\'m lerning englsh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1df4bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_text = spell.unknown(text.split())\n",
    "    print(f'Misspelled word : {misspelled_text}')\n",
    "    for word in text.split():\n",
    "        if word in misspelled_text:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1c6bbded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misspelled word : {'lerning', 'helllo,', 'englsh', 'arafat.', 'm'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"hello i am Arafat. i'm learning english\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_spellings(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c06c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e97f0710",
   "metadata": {},
   "source": [
    "# Feature Extraction from Text Data\n",
    "### Bag of Words\n",
    "\n",
    "A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things: A vocabulary of known words. A measure of the presence of known words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8539bea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = ['I am interested in NLP', 'This is a good tutorial with good topic', 'Feature extraction is very important topic']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "218c573b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a51985a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(stop_words='english')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the data\n",
    "bow.fit(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3ba534ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arafat/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['extraction',\n",
       " 'feature',\n",
       " 'good',\n",
       " 'important',\n",
       " 'interested',\n",
       " 'nlp',\n",
       " 'topic',\n",
       " 'tutorial']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vocabulary list\n",
    "bow.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "27bf8b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x8 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_features = bow.transform(text_data)\n",
    "bow_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f31b4c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 2, 0, 0, 0, 1, 1],\n",
       "       [1, 1, 0, 1, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_feature_array = bow_features.toarray()\n",
    "bow_feature_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b65bbe04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['extraction', 'feature', 'good', 'important', 'interested', 'nlp', 'topic', 'tutorial']\n",
      "I am interested in NLP\n",
      "[0 0 0 0 1 1 0 0]\n",
      "This is a good tutorial with good topic\n",
      "[0 0 2 0 0 0 1 1]\n",
      "Feature extraction is very important topic\n",
      "[1 1 0 1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(bow.get_feature_names())\n",
    "for sentence, feature in zip(text_data, bow_feature_array):\n",
    "    print(sentence)\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8a25ca41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 8)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_feature_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92de01a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c0401cf",
   "metadata": {},
   "source": [
    "# TF-IDF (Term Frequency/Inverse Document Frequency)\n",
    "\n",
    "TF-IDF stands for term frequency-inverse document frequency and it is a measure, used in the fields of information retrieval (IR) and machine learning, that can quantify the importance or relevance of string representations (words, phrases, lemmas, etc) in a document amongst a collection of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "79716909",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = ['I am interested in NLP', 'This is a good tutorial with good topic', 'Feature extraction is very important topic']\n",
    "text_data = ['good boy','good girl', 'boy girl good']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8d251189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eeeca2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(stop_words='english')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the data\n",
    "tfidf.fit(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "34af8b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'good': 2, 'boy': 0, 'girl': 1}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vocabulary list\n",
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f10a305f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x3 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_features = tfidf.transform(text_data)\n",
    "tfidf_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c5a1164f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.78980693, 0.        , 0.61335554],\n",
       "       [0.        , 0.78980693, 0.61335554],\n",
       "       [0.61980538, 0.61980538, 0.48133417]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_feature_array = tfidf_features.toarray()\n",
    "tfidf_feature_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "371cbfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good boy\n",
      "  (0, 2)\t0.6133555370249717\n",
      "  (0, 0)\t0.7898069290660905\n",
      "good girl\n",
      "  (0, 2)\t0.6133555370249717\n",
      "  (0, 1)\t0.7898069290660905\n",
      "boy girl good\n",
      "  (0, 2)\t0.48133416873660545\n",
      "  (0, 1)\t0.6198053799406072\n",
      "  (0, 0)\t0.6198053799406072\n"
     ]
    }
   ],
   "source": [
    "for sentence, feature in zip(text_data, tfidf_features):\n",
    "    print(sentence)\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ebafa4",
   "metadata": {},
   "source": [
    "# Word2vec\n",
    "The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8dfd5a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /home/arafat/anaconda3/lib/python3.9/site-packages (4.2.0)\r\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/arafat/anaconda3/lib/python3.9/site-packages (from gensim) (1.7.3)\r\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/arafat/anaconda3/lib/python3.9/site-packages (from gensim) (1.19.5)\r\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/arafat/anaconda3/lib/python3.9/site-packages (from gensim) (5.2.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "17f46b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ad6c144f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text data\n",
    "common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "364cb454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(common_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b3a5c620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and fit the data\n",
    "model = Word2Vec(common_texts, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "57ab3e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.6196875e-03,  3.6657380e-03,  5.1898835e-03,  5.7419371e-03,\n",
       "        7.4669169e-03, -6.1676763e-03,  1.1056137e-03,  6.0472824e-03,\n",
       "       -2.8400517e-03, -6.1735227e-03, -4.1022300e-04, -8.3689503e-03,\n",
       "       -5.6000138e-03,  7.1045374e-03,  3.3525396e-03,  7.2256685e-03,\n",
       "        6.8002464e-03,  7.5307419e-03, -3.7891555e-03, -5.6180713e-04,\n",
       "        2.3483753e-03, -4.5190332e-03,  8.3887316e-03, -9.8581649e-03,\n",
       "        6.7646410e-03,  2.9144168e-03, -4.9328329e-03,  4.3981862e-03,\n",
       "       -1.7395759e-03,  6.7113829e-03,  9.9648498e-03, -4.3624449e-03,\n",
       "       -5.9933902e-04, -5.6956387e-03,  3.8508223e-03,  2.7866268e-03,\n",
       "        6.8910765e-03,  6.1010956e-03,  9.5384959e-03,  9.2734173e-03,\n",
       "        7.8980681e-03, -6.9895051e-03, -9.1558648e-03, -3.5575390e-04,\n",
       "       -3.0998420e-03,  7.8943158e-03,  5.9385728e-03, -1.5456629e-03,\n",
       "        1.5109634e-03,  1.7900396e-03,  7.8175711e-03, -9.5101884e-03,\n",
       "       -2.0553112e-04,  3.4691954e-03, -9.3897345e-04,  8.3817719e-03,\n",
       "        9.0107825e-03,  6.5365052e-03, -7.1162224e-04,  7.7104042e-03,\n",
       "       -8.5343365e-03,  3.2071066e-03, -4.6379971e-03, -5.0889566e-03,\n",
       "        3.5896183e-03,  5.3703380e-03,  7.7695129e-03, -5.7665063e-03,\n",
       "        7.4333595e-03,  6.6254949e-03, -3.7098003e-03, -8.7456414e-03,\n",
       "        5.4374672e-03,  6.5097548e-03, -7.8755140e-04, -6.7098569e-03,\n",
       "       -7.0859264e-03, -2.4970602e-03,  5.1432536e-03, -3.6652375e-03,\n",
       "       -9.3700597e-03,  3.8267397e-03,  4.8844791e-03, -6.4285635e-03,\n",
       "        1.2085581e-03, -2.0748782e-03,  2.4402141e-05, -9.8835090e-03,\n",
       "        2.6920033e-03, -4.7501065e-03,  1.0876465e-03, -1.5762257e-03,\n",
       "        2.1966719e-03, -7.8815771e-03, -2.7171851e-03,  2.6631975e-03,\n",
       "        5.3466819e-03, -2.3915148e-03, -9.5100952e-03,  4.5058774e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['graph']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "132a4e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('user', 0.06793875992298126),\n",
       " ('survey', 0.03364057466387749),\n",
       " ('eps', 0.009391184896230698),\n",
       " ('human', 0.008315940387547016),\n",
       " ('minors', 0.004503006115555763),\n",
       " ('system', -0.010839183814823627),\n",
       " ('trees', -0.023671666160225868),\n",
       " ('computer', -0.09575347602367401),\n",
       " ('time', -0.11410722136497498),\n",
       " ('response', -0.11557211726903915)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dc3b67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
